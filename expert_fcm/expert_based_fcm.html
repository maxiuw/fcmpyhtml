<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>fcmpy.expert_fcm.expert_based_fcm API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>fcmpy.expert_fcm.expert_based_fcm</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np 
import pandas as pd
import functools
import collections
from abc import ABC, abstractmethod
from fcmpy.expert_fcm.input_validator import type_check
from fcmpy.store.methodsStore import EntropyStore
from fcmpy.store.methodsStore import ReaderStore
from fcmpy.store.methodsStore import MembershipStore
from fcmpy.store.methodsStore import ImplicationStore
from fcmpy.store.methodsStore import AggregationStore
from fcmpy.store.methodsStore import DefuzzStore
from fcmpy.expert_fcm.transform import Transform


class FcmConstructor(ABC):
    @abstractmethod
    def read_data(file_path, **kwargs) -&gt; collections.OrderedDict:
        raise NotImplementedError(&#39;read_data method is not defined!&#39;)

    @abstractmethod
    def build(data: collections.OrderedDict, implication_method:str, 
                    aggregation_method:str, defuzz_method:str) -&gt; pd.DataFrame:
                    
        raise NotImplementedError(&#39;build method is not defined!&#39;)


class ExpertFcm(FcmConstructor):
    &#34;&#34;&#34;
        Construct Expert FCMs based on qualitative input data.

        Methods:
            read_data(file_path, **kwargs)
            entropy(data: collections.OrderedDict, method = &#39;entropy&#39;, **kwargs)
            automf(method:str=&#39;trimf&#39;, **kwargs)
            fuzzy_implication(membership_function, weight, method:str=&#39;Mamdani&#39;, **kwargs)
            aggregate(x, y, method:str=&#39;fMax&#39;, **kwargs)
            defuzz(x, mfx, method:str=&#39;centroid&#39;, **kwargs)
            build(data: collections.OrderedDict, implication_method:str=&#39;Mamdani&#39;, 
                            aggregation_method:str=&#39;fMax&#39;, defuzz_method:str=&#39;centroid&#39;)
    &#34;&#34;&#34;
    @type_check
    def __init__(self):
        self.__linguisticTerms = None
        self.__membership = None
        self.__universe = None

    @property
    def linguistic_terms(self):
        return self.__linguisticTerms

    @linguistic_terms.setter
    @type_check
    def linguistic_terms(self, terms: dict):
        &#34;&#34;&#34;
            Linguistic terms and the associated parameters for generating fuzzy membership functions.

            Parameters
            ----------
            terms : dict,
                        keys are the linguistic terms and the values are 
                        lists with term parameters (see blow)

            Term parameters
            ----------------
            for trimf:
                the parameters &#39;abc&#39; should be passed as keys (in a list)
                to the linguistic terms -&gt; e.g., {&#39;+VL&#39;: [0.25, 0.5, 0.75]}
            
            for gaussmf:
                the parameters &#39;mean&#39; and &#39;sigma&#39; should be passed as keys (in a list)
                to the linguistic terms -&gt; e.g., {&#39;+VL&#39;: [0.25, 0.1]}
            
            for trapmf:
                the parameters &#39;abcd&#39; should be passed as keys to the linguistic terms
                e.g., {&#39;+VL&#39;: [0, 0.25, 0.5, 0.75]}
        &#34;&#34;&#34;
        termsLower = dict((k.lower(), v) for k, v in terms.items()) 
        self.__linguisticTerms = termsLower

    @property
    def fuzzy_membership(self):
        return self.__membership
    
    @fuzzy_membership.setter
    @type_check
    def fuzzy_membership(self, membership:dict):   
        &#34;&#34;&#34;
            Parameters
            ----------
            membership : dict,
                            keys are the linguistic terms and the values are the associated
                            numpy arrays with membership values.
        &#34;&#34;&#34;

        self.__membership = membership

    @property
    def universe(self):
        return self.__universe
    
    @universe.setter
    @type_check
    def universe(self, universe:np.ndarray):
        
        &#34;&#34;&#34;
        Parameters
        ----------
        universe: np.ndarray
                    the universe of discourse. 
                    (Note that the generated arrays are automatically rounded to 2 digits)
        &#34;&#34;&#34;

        self.__universe = universe.round(2)

    @type_check
    def read_data(self, file_path:str, **kwargs) -&gt; collections.OrderedDict:
        
        &#34;&#34;&#34; 
            Read data from a file. Currently, the method supports 
            .csv, .xlsx and .json file formats.
            
            Parameters
            ----------
            file_path : str 
            
            Other Parameters
            ----------------
            for .csv files:

                **sep_concept: str,
                                separation symbol (e.g., &#39;-&gt;&#39;) that separates the antecedent 
                                from the consequent in the column heads of a csv file
                                default ---&gt; &#39;-&gt;&#39;

                **csv_sep: str,
                            separator of the csv file
                            default ---&gt; &#39;,&#39;
                            
            for .xlsx files:

                **check_consistency: Bool
                                    check the consistency of raitings across the experts
                                    default --&gt; False

                **engine: str,
                            the engine for excel reader (read more in pd.read_excel)
                            default --&gt; &#34;openpyxl&#34;

            for .json files:

                **check_consistency: Bool
                                    check the consistency of raitings across the experts.
                                    default --&gt; False
            
            Return
            -------
            data: collections.OrderedDict
                    ordered dictionary with the formatted data.
        &#34;&#34;&#34;

        if self.linguistic_terms is None:
            raise ValueError(&#39;Linguistic terms are not defined!&#39;)

        fileType = file_path.split(&#39;.&#39;)[-1]
        reader = ReaderStore.get(fileType)()
        data = reader.read(filePath=file_path, linguisticTerms = self.linguistic_terms,
                            params = kwargs)
        
        return data

    @type_check
    def entropy(self, data: collections.OrderedDict, 
                method:str = &#39;entropy&#39;, **kwargs) -&gt; pd.DataFrame:
        &#34;&#34;&#34;
            Calculate the entropy of the expert ratings.

            Parameters
            ----------
            data: collections.OrderedDict
                ordered dictionary with the expert inputs
            
            method: str
                    method for calculating entropy. At the moment only information entropy is available
                    default --&gt; &#39;entropy&#39;

            Return
            -------
            y: pandas.DataFrame,
                entropy of the concept pairs in expert ratings
        &#34;&#34;&#34;

        entropy = EntropyStore.get(method)()
        return entropy.calculateEntropy(data=data, params=kwargs)
    
    @type_check
    def automf(self, method:str=&#39;trimf&#39;, **kwargs) -&gt; dict:
        &#34;&#34;&#34;
            Generate membership functions for a given set of linguistic terms.

            Parameters
            ----------
            method: str
                    type of membership function. At the moment three such types are available  
                    &#39;trimf&#39;: triangular membership functions
                    &#39;gaussmf&#39;: gaussian membership functions
                    &#39;trapmf&#39;: trapezoidal membership functions
                    default ---&gt; &#39;trimf&#39;
            
            Return
            -------
            y: dict
                generated membership functions. The keys are the linguistic
                terms and the values are 1d arrays
        &#34;&#34;&#34;
        if self.linguistic_terms is None:
            raise ValueError(&#39;Linguistic terms are not defined!&#39;)
        elif self.universe is None:
            raise ValueError(&#39;Universe of discourse is not defined!&#39;)
        
        membership = MembershipStore.get(method)()

        return membership.membershipFunction(linguistic_terms=self.linguistic_terms, 
                                            universe=self.universe, params = kwargs)
    
    @type_check
    def fuzzy_implication(self, membership_function, weight, 
                            method:str=&#39;Mamdani&#39;, **kwargs) -&gt; np.ndarray:
        &#34;&#34;&#34;
            Fuzzy implication rule.

            Parameters
            ----------
            membership_function: numpy.ndarray,
                                    membership function of a linguistic term (x)

            weight: float,
                        the weight at which the given membership function x should be &#34;activated&#34; 
                        (i.e., the cut point or the point at which the membership function should be rescaled)
            
            method: str,
                    implication rule; at the moment two such rules are available 
                    &#39;Mamdani&#39;: minimum implication rule
                    &#39;Larsen&#39;: product implication rule
                    default ---&gt; &#39;Mamdani&#39;

            Return
            -------
            y: numpy.ndarray
                the &#34;activated&#34; membership function
        &#34;&#34;&#34;
        implication = ImplicationStore.get(method)()

        return implication.implication(mf_x = membership_function, 
                                        weight = weight, params = kwargs)

    @type_check
    def aggregate(self, x, y, method:str=&#39;fMax&#39;, **kwargs) -&gt; np.ndarray:
        &#34;&#34;&#34;
            Fuzzy aggregation rule.

            Parameters
            ----------
            x, y: numpy.ndarray,
                        &#34;activated&#34; membership functions of the linguistic 
                        terms that need to be aggregated
            
            method: str,
                    aggregation rule; at the moment four such rules are available
                    &#39;fMax&#39;: family maximum,
                    &#39;algSum&#39;: family Algebraic Sum,
                    &#39;eSum&#39;: family Einstein Sum,
                    &#39;hSum&#39;: family Hamacher Sum
                    default ---&gt; &#39;fMax&#39;

            Return
            -------
            y: numpy.ndarray
                an aggregated membership function
        &#34;&#34;&#34;

        aggr = AggregationStore.get(method)()
        aggregated = aggr.aggregate(x=x, y=y, params = kwargs)
        return aggregated

    @type_check
    def defuzz(self, x, mfx, method:str=&#39;centroid&#39;, **kwargs) -&gt; float:
        &#34;&#34;&#34;
            Defuzzification of the aggregated membership functions.

            Parameters
            ----------
            x: numpy.ndarray
                universe of discourse 
            
            mfx: numpy.ndarray,
                        &#34;aggregated&#34; membership functions
            
            method: str,
                    defuzzification method; at the moment four such 
                    rules are available:
                        &#39;centroid&#39;: Centroid,
                        &#39;bisector&#39;: Bisector,
                        &#39;mom&#39;: MeanOfMax,
                        &#39;som&#39;: MinOfMax,
                        &#39;lom&#39; : MaxOfMax
                        default ---&gt; &#39;centroid&#39;

            Return
            -------
            y: float
                defuzzified value
        &#34;&#34;&#34;

        defuzz = DefuzzStore.get(method)()
        defuzz_value = defuzz.defuzz(x=x, mfx=mfx, method=method, params=kwargs)

        return defuzz_value

    @type_check
    def build(self, data: collections.OrderedDict, implication_method:str=&#39;Mamdani&#39;, 
                    aggregation_method:str=&#39;fMax&#39;, defuzz_method:str=&#39;centroid&#39;) -&gt; pd.DataFrame:
        &#34;&#34;&#34;
            Build an FCM based on qualitative input data.

            Parameters
            ----------
            data: collections.OrderedDict
                    ordered dictionary with the qualitative input data.
            
            implication_method: str,
                                implication rule; at the moment two such
                                rules are available;
                                    &#39;Mamdani&#39;: minimum implication rule
                                    &#39;Larsen&#39;: product implication rule
                                    default ---&gt; &#39;Mamdani&#39;

            aggregation_method: str,
                                aggregation rule; at the moment four such
                                rules are available:
                                    &#39;fMax&#39;: family maximum,
                                    &#39;algSum&#39;: family Algebraic Sum,
                                    &#39;eSum&#39;: family Einstein Sum,
                                    &#39;hSum&#39;: family Hamacher Sum
                                    default ---&gt; &#39;fMax&#39;

            defuzz_method: str,            
                            defuzzification method; at the moment four such
                            rules are available:
                                &#39;centroid&#39;: Centroid,
                                &#39;bisector&#39;: Bisector,
                                &#39;mom&#39;: MeanOfMax,
                                &#39;som&#39;: MinOfMax,
                                &#39;lom&#39; : MaxOfMax
                                default ---&gt; &#39;centroid&#39;

            Return
            -------
            y: pd.DataFrame
                the connection matrix with the defuzzified values            
        &#34;&#34;&#34;
        if self.fuzzy_membership is None:
            raise ValueError(&#39;Membership function is not defined!&#39;)

        if self.linguistic_terms is None:
            raise ValueError(&#39;linguistic_terms are not defined!&#39;)

        if self.universe is None:
            raise ValueError(&#39;Universe of discourse is not defined!&#39;)

        nExperts = len(data)

        # Drop the columns that should be omitted from the calculations (e.g., unsure)
        keep = [i.lower() for i in list(self.linguistic_terms.keys())]
        flat_data = Transform.flatData(data)
        flat_data = flat_data[keep]
        
        # Create an empty weight matrix
        cols = set(flat_data.index.get_level_values(&#39;to&#39;))
        index = set(flat_data.index.get_level_values(&#39;from&#39;))
        index = sorted(index.union(cols))
        weight_matrix = pd.DataFrame(0,columns=index, index=index)
        
        # main part for calculating the weights
        for concepts in set(flat_data.index):
            # for a given pair of concepts calculate the propostions (weights) for the
            # implication rules.
            activation_parameter = Transform.calculateProportions(data=flat_data, 
                                            conceptPair=concepts, nExperts=nExperts)
            activated = {}
            # for each linguistic term apply the implication rule
            for term in self.fuzzy_membership.keys():
                act = self.fuzzy_implication(membership_function=self.fuzzy_membership[term], 
                                            weight=activation_parameter[term], method=implication_method)
                activated[term] = act
            
            # if the &#39;activated&#39; membership functions are not all zeros then aggregate 
            # them and defuzzify them.
            if not all(x==0 for x in activation_parameter.values()):
                # aggregate all the activated membership functions
                aggregated = functools.reduce(lambda x,y: self.aggregate(x=x, y=y, method=aggregation_method),
                                                [activated[i] for i in activated.keys()])

                # defuzzify the aggregated functions                                
                value = self.defuzz(x=self.universe, mfx=aggregated, method=defuzz_method)
                # populate the empty weigtht_matrix with the defuzzified value
                weight_matrix.loc[concepts] = value
        
        weight_matrix = weight_matrix.fillna(0)

        return weight_matrix</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="fcmpy.expert_fcm.expert_based_fcm.ExpertFcm"><code class="flex name class">
<span>class <span class="ident">ExpertFcm</span></span>
</code></dt>
<dd>
<div class="desc"><p>Construct Expert FCMs based on qualitative input data.</p>
<h2 id="methods">Methods</h2>
<p>read_data(file_path, <strong>kwargs)
entropy(data: collections.OrderedDict, method = 'entropy', </strong>kwargs)
automf(method:str='trimf', <strong>kwargs)
fuzzy_implication(membership_function, weight, method:str='Mamdani', </strong>kwargs)
aggregate(x, y, method:str='fMax', <strong>kwargs)
defuzz(x, mfx, method:str='centroid', </strong>kwargs)
build(data: collections.OrderedDict, implication_method:str='Mamdani',
aggregation_method:str='fMax', defuzz_method:str='centroid')</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ExpertFcm(FcmConstructor):
    &#34;&#34;&#34;
        Construct Expert FCMs based on qualitative input data.

        Methods:
            read_data(file_path, **kwargs)
            entropy(data: collections.OrderedDict, method = &#39;entropy&#39;, **kwargs)
            automf(method:str=&#39;trimf&#39;, **kwargs)
            fuzzy_implication(membership_function, weight, method:str=&#39;Mamdani&#39;, **kwargs)
            aggregate(x, y, method:str=&#39;fMax&#39;, **kwargs)
            defuzz(x, mfx, method:str=&#39;centroid&#39;, **kwargs)
            build(data: collections.OrderedDict, implication_method:str=&#39;Mamdani&#39;, 
                            aggregation_method:str=&#39;fMax&#39;, defuzz_method:str=&#39;centroid&#39;)
    &#34;&#34;&#34;
    @type_check
    def __init__(self):
        self.__linguisticTerms = None
        self.__membership = None
        self.__universe = None

    @property
    def linguistic_terms(self):
        return self.__linguisticTerms

    @linguistic_terms.setter
    @type_check
    def linguistic_terms(self, terms: dict):
        &#34;&#34;&#34;
            Linguistic terms and the associated parameters for generating fuzzy membership functions.

            Parameters
            ----------
            terms : dict,
                        keys are the linguistic terms and the values are 
                        lists with term parameters (see blow)

            Term parameters
            ----------------
            for trimf:
                the parameters &#39;abc&#39; should be passed as keys (in a list)
                to the linguistic terms -&gt; e.g., {&#39;+VL&#39;: [0.25, 0.5, 0.75]}
            
            for gaussmf:
                the parameters &#39;mean&#39; and &#39;sigma&#39; should be passed as keys (in a list)
                to the linguistic terms -&gt; e.g., {&#39;+VL&#39;: [0.25, 0.1]}
            
            for trapmf:
                the parameters &#39;abcd&#39; should be passed as keys to the linguistic terms
                e.g., {&#39;+VL&#39;: [0, 0.25, 0.5, 0.75]}
        &#34;&#34;&#34;
        termsLower = dict((k.lower(), v) for k, v in terms.items()) 
        self.__linguisticTerms = termsLower

    @property
    def fuzzy_membership(self):
        return self.__membership
    
    @fuzzy_membership.setter
    @type_check
    def fuzzy_membership(self, membership:dict):   
        &#34;&#34;&#34;
            Parameters
            ----------
            membership : dict,
                            keys are the linguistic terms and the values are the associated
                            numpy arrays with membership values.
        &#34;&#34;&#34;

        self.__membership = membership

    @property
    def universe(self):
        return self.__universe
    
    @universe.setter
    @type_check
    def universe(self, universe:np.ndarray):
        
        &#34;&#34;&#34;
        Parameters
        ----------
        universe: np.ndarray
                    the universe of discourse. 
                    (Note that the generated arrays are automatically rounded to 2 digits)
        &#34;&#34;&#34;

        self.__universe = universe.round(2)

    @type_check
    def read_data(self, file_path:str, **kwargs) -&gt; collections.OrderedDict:
        
        &#34;&#34;&#34; 
            Read data from a file. Currently, the method supports 
            .csv, .xlsx and .json file formats.
            
            Parameters
            ----------
            file_path : str 
            
            Other Parameters
            ----------------
            for .csv files:

                **sep_concept: str,
                                separation symbol (e.g., &#39;-&gt;&#39;) that separates the antecedent 
                                from the consequent in the column heads of a csv file
                                default ---&gt; &#39;-&gt;&#39;

                **csv_sep: str,
                            separator of the csv file
                            default ---&gt; &#39;,&#39;
                            
            for .xlsx files:

                **check_consistency: Bool
                                    check the consistency of raitings across the experts
                                    default --&gt; False

                **engine: str,
                            the engine for excel reader (read more in pd.read_excel)
                            default --&gt; &#34;openpyxl&#34;

            for .json files:

                **check_consistency: Bool
                                    check the consistency of raitings across the experts.
                                    default --&gt; False
            
            Return
            -------
            data: collections.OrderedDict
                    ordered dictionary with the formatted data.
        &#34;&#34;&#34;

        if self.linguistic_terms is None:
            raise ValueError(&#39;Linguistic terms are not defined!&#39;)

        fileType = file_path.split(&#39;.&#39;)[-1]
        reader = ReaderStore.get(fileType)()
        data = reader.read(filePath=file_path, linguisticTerms = self.linguistic_terms,
                            params = kwargs)
        
        return data

    @type_check
    def entropy(self, data: collections.OrderedDict, 
                method:str = &#39;entropy&#39;, **kwargs) -&gt; pd.DataFrame:
        &#34;&#34;&#34;
            Calculate the entropy of the expert ratings.

            Parameters
            ----------
            data: collections.OrderedDict
                ordered dictionary with the expert inputs
            
            method: str
                    method for calculating entropy. At the moment only information entropy is available
                    default --&gt; &#39;entropy&#39;

            Return
            -------
            y: pandas.DataFrame,
                entropy of the concept pairs in expert ratings
        &#34;&#34;&#34;

        entropy = EntropyStore.get(method)()
        return entropy.calculateEntropy(data=data, params=kwargs)
    
    @type_check
    def automf(self, method:str=&#39;trimf&#39;, **kwargs) -&gt; dict:
        &#34;&#34;&#34;
            Generate membership functions for a given set of linguistic terms.

            Parameters
            ----------
            method: str
                    type of membership function. At the moment three such types are available  
                    &#39;trimf&#39;: triangular membership functions
                    &#39;gaussmf&#39;: gaussian membership functions
                    &#39;trapmf&#39;: trapezoidal membership functions
                    default ---&gt; &#39;trimf&#39;
            
            Return
            -------
            y: dict
                generated membership functions. The keys are the linguistic
                terms and the values are 1d arrays
        &#34;&#34;&#34;
        if self.linguistic_terms is None:
            raise ValueError(&#39;Linguistic terms are not defined!&#39;)
        elif self.universe is None:
            raise ValueError(&#39;Universe of discourse is not defined!&#39;)
        
        membership = MembershipStore.get(method)()

        return membership.membershipFunction(linguistic_terms=self.linguistic_terms, 
                                            universe=self.universe, params = kwargs)
    
    @type_check
    def fuzzy_implication(self, membership_function, weight, 
                            method:str=&#39;Mamdani&#39;, **kwargs) -&gt; np.ndarray:
        &#34;&#34;&#34;
            Fuzzy implication rule.

            Parameters
            ----------
            membership_function: numpy.ndarray,
                                    membership function of a linguistic term (x)

            weight: float,
                        the weight at which the given membership function x should be &#34;activated&#34; 
                        (i.e., the cut point or the point at which the membership function should be rescaled)
            
            method: str,
                    implication rule; at the moment two such rules are available 
                    &#39;Mamdani&#39;: minimum implication rule
                    &#39;Larsen&#39;: product implication rule
                    default ---&gt; &#39;Mamdani&#39;

            Return
            -------
            y: numpy.ndarray
                the &#34;activated&#34; membership function
        &#34;&#34;&#34;
        implication = ImplicationStore.get(method)()

        return implication.implication(mf_x = membership_function, 
                                        weight = weight, params = kwargs)

    @type_check
    def aggregate(self, x, y, method:str=&#39;fMax&#39;, **kwargs) -&gt; np.ndarray:
        &#34;&#34;&#34;
            Fuzzy aggregation rule.

            Parameters
            ----------
            x, y: numpy.ndarray,
                        &#34;activated&#34; membership functions of the linguistic 
                        terms that need to be aggregated
            
            method: str,
                    aggregation rule; at the moment four such rules are available
                    &#39;fMax&#39;: family maximum,
                    &#39;algSum&#39;: family Algebraic Sum,
                    &#39;eSum&#39;: family Einstein Sum,
                    &#39;hSum&#39;: family Hamacher Sum
                    default ---&gt; &#39;fMax&#39;

            Return
            -------
            y: numpy.ndarray
                an aggregated membership function
        &#34;&#34;&#34;

        aggr = AggregationStore.get(method)()
        aggregated = aggr.aggregate(x=x, y=y, params = kwargs)
        return aggregated

    @type_check
    def defuzz(self, x, mfx, method:str=&#39;centroid&#39;, **kwargs) -&gt; float:
        &#34;&#34;&#34;
            Defuzzification of the aggregated membership functions.

            Parameters
            ----------
            x: numpy.ndarray
                universe of discourse 
            
            mfx: numpy.ndarray,
                        &#34;aggregated&#34; membership functions
            
            method: str,
                    defuzzification method; at the moment four such 
                    rules are available:
                        &#39;centroid&#39;: Centroid,
                        &#39;bisector&#39;: Bisector,
                        &#39;mom&#39;: MeanOfMax,
                        &#39;som&#39;: MinOfMax,
                        &#39;lom&#39; : MaxOfMax
                        default ---&gt; &#39;centroid&#39;

            Return
            -------
            y: float
                defuzzified value
        &#34;&#34;&#34;

        defuzz = DefuzzStore.get(method)()
        defuzz_value = defuzz.defuzz(x=x, mfx=mfx, method=method, params=kwargs)

        return defuzz_value

    @type_check
    def build(self, data: collections.OrderedDict, implication_method:str=&#39;Mamdani&#39;, 
                    aggregation_method:str=&#39;fMax&#39;, defuzz_method:str=&#39;centroid&#39;) -&gt; pd.DataFrame:
        &#34;&#34;&#34;
            Build an FCM based on qualitative input data.

            Parameters
            ----------
            data: collections.OrderedDict
                    ordered dictionary with the qualitative input data.
            
            implication_method: str,
                                implication rule; at the moment two such
                                rules are available;
                                    &#39;Mamdani&#39;: minimum implication rule
                                    &#39;Larsen&#39;: product implication rule
                                    default ---&gt; &#39;Mamdani&#39;

            aggregation_method: str,
                                aggregation rule; at the moment four such
                                rules are available:
                                    &#39;fMax&#39;: family maximum,
                                    &#39;algSum&#39;: family Algebraic Sum,
                                    &#39;eSum&#39;: family Einstein Sum,
                                    &#39;hSum&#39;: family Hamacher Sum
                                    default ---&gt; &#39;fMax&#39;

            defuzz_method: str,            
                            defuzzification method; at the moment four such
                            rules are available:
                                &#39;centroid&#39;: Centroid,
                                &#39;bisector&#39;: Bisector,
                                &#39;mom&#39;: MeanOfMax,
                                &#39;som&#39;: MinOfMax,
                                &#39;lom&#39; : MaxOfMax
                                default ---&gt; &#39;centroid&#39;

            Return
            -------
            y: pd.DataFrame
                the connection matrix with the defuzzified values            
        &#34;&#34;&#34;
        if self.fuzzy_membership is None:
            raise ValueError(&#39;Membership function is not defined!&#39;)

        if self.linguistic_terms is None:
            raise ValueError(&#39;linguistic_terms are not defined!&#39;)

        if self.universe is None:
            raise ValueError(&#39;Universe of discourse is not defined!&#39;)

        nExperts = len(data)

        # Drop the columns that should be omitted from the calculations (e.g., unsure)
        keep = [i.lower() for i in list(self.linguistic_terms.keys())]
        flat_data = Transform.flatData(data)
        flat_data = flat_data[keep]
        
        # Create an empty weight matrix
        cols = set(flat_data.index.get_level_values(&#39;to&#39;))
        index = set(flat_data.index.get_level_values(&#39;from&#39;))
        index = sorted(index.union(cols))
        weight_matrix = pd.DataFrame(0,columns=index, index=index)
        
        # main part for calculating the weights
        for concepts in set(flat_data.index):
            # for a given pair of concepts calculate the propostions (weights) for the
            # implication rules.
            activation_parameter = Transform.calculateProportions(data=flat_data, 
                                            conceptPair=concepts, nExperts=nExperts)
            activated = {}
            # for each linguistic term apply the implication rule
            for term in self.fuzzy_membership.keys():
                act = self.fuzzy_implication(membership_function=self.fuzzy_membership[term], 
                                            weight=activation_parameter[term], method=implication_method)
                activated[term] = act
            
            # if the &#39;activated&#39; membership functions are not all zeros then aggregate 
            # them and defuzzify them.
            if not all(x==0 for x in activation_parameter.values()):
                # aggregate all the activated membership functions
                aggregated = functools.reduce(lambda x,y: self.aggregate(x=x, y=y, method=aggregation_method),
                                                [activated[i] for i in activated.keys()])

                # defuzzify the aggregated functions                                
                value = self.defuzz(x=self.universe, mfx=aggregated, method=defuzz_method)
                # populate the empty weigtht_matrix with the defuzzified value
                weight_matrix.loc[concepts] = value
        
        weight_matrix = weight_matrix.fillna(0)

        return weight_matrix</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="fcmpy.expert_fcm.expert_based_fcm.FcmConstructor" href="#fcmpy.expert_fcm.expert_based_fcm.FcmConstructor">FcmConstructor</a></li>
<li>abc.ABC</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.fuzzy_membership"><code class="name">var <span class="ident">fuzzy_membership</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def fuzzy_membership(self):
    return self.__membership</code></pre>
</details>
</dd>
<dt id="fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.linguistic_terms"><code class="name">var <span class="ident">linguistic_terms</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def linguistic_terms(self):
    return self.__linguisticTerms</code></pre>
</details>
</dd>
<dt id="fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.universe"><code class="name">var <span class="ident">universe</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def universe(self):
    return self.__universe</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.aggregate"><code class="name flex">
<span>def <span class="ident">aggregate</span></span>(<span>self, x, y, method: str = 'fMax', **kwargs) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Fuzzy aggregation rule.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong>, <strong><code>y</code></strong> :&ensp;<code>numpy.ndarray,</code></dt>
<dd>"activated" membership functions of the linguistic
terms that need to be aggregated</dd>
<dt><strong><code>method</code></strong> :&ensp;<code>str,</code></dt>
<dd>aggregation rule; at the moment four such rules are available
'fMax': family maximum,
'algSum': family Algebraic Sum,
'eSum': family Einstein Sum,
'hSum': family Hamacher Sum
default &mdash;&gt; 'fMax'</dd>
</dl>
<h2 id="return">Return</h2>
<p>y: numpy.ndarray
an aggregated membership function</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@type_check
def aggregate(self, x, y, method:str=&#39;fMax&#39;, **kwargs) -&gt; np.ndarray:
    &#34;&#34;&#34;
        Fuzzy aggregation rule.

        Parameters
        ----------
        x, y: numpy.ndarray,
                    &#34;activated&#34; membership functions of the linguistic 
                    terms that need to be aggregated
        
        method: str,
                aggregation rule; at the moment four such rules are available
                &#39;fMax&#39;: family maximum,
                &#39;algSum&#39;: family Algebraic Sum,
                &#39;eSum&#39;: family Einstein Sum,
                &#39;hSum&#39;: family Hamacher Sum
                default ---&gt; &#39;fMax&#39;

        Return
        -------
        y: numpy.ndarray
            an aggregated membership function
    &#34;&#34;&#34;

    aggr = AggregationStore.get(method)()
    aggregated = aggr.aggregate(x=x, y=y, params = kwargs)
    return aggregated</code></pre>
</details>
</dd>
<dt id="fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.automf"><code class="name flex">
<span>def <span class="ident">automf</span></span>(<span>self, method: str = 'trimf', **kwargs) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Generate membership functions for a given set of linguistic terms.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>method</code></strong> :&ensp;<code>str</code></dt>
<dd>type of membership function. At the moment three such types are available<br>
'trimf': triangular membership functions
'gaussmf': gaussian membership functions
'trapmf': trapezoidal membership functions
default &mdash;&gt; 'trimf'</dd>
</dl>
<h2 id="return">Return</h2>
<p>y: dict
generated membership functions. The keys are the linguistic
terms and the values are 1d arrays</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@type_check
def automf(self, method:str=&#39;trimf&#39;, **kwargs) -&gt; dict:
    &#34;&#34;&#34;
        Generate membership functions for a given set of linguistic terms.

        Parameters
        ----------
        method: str
                type of membership function. At the moment three such types are available  
                &#39;trimf&#39;: triangular membership functions
                &#39;gaussmf&#39;: gaussian membership functions
                &#39;trapmf&#39;: trapezoidal membership functions
                default ---&gt; &#39;trimf&#39;
        
        Return
        -------
        y: dict
            generated membership functions. The keys are the linguistic
            terms and the values are 1d arrays
    &#34;&#34;&#34;
    if self.linguistic_terms is None:
        raise ValueError(&#39;Linguistic terms are not defined!&#39;)
    elif self.universe is None:
        raise ValueError(&#39;Universe of discourse is not defined!&#39;)
    
    membership = MembershipStore.get(method)()

    return membership.membershipFunction(linguistic_terms=self.linguistic_terms, 
                                        universe=self.universe, params = kwargs)</code></pre>
</details>
</dd>
<dt id="fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.build"><code class="name flex">
<span>def <span class="ident">build</span></span>(<span>self, data: collections.OrderedDict, implication_method: str = 'Mamdani', aggregation_method: str = 'fMax', defuzz_method: str = 'centroid') ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Build an FCM based on qualitative input data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>collections.OrderedDict</code></dt>
<dd>ordered dictionary with the qualitative input data.</dd>
<dt><strong><code>implication_method</code></strong> :&ensp;<code>str,</code></dt>
<dd>implication rule; at the moment two such
rules are available;
'Mamdani': minimum implication rule
'Larsen': product implication rule
default &mdash;&gt; 'Mamdani'</dd>
<dt><strong><code>aggregation_method</code></strong> :&ensp;<code>str,</code></dt>
<dd>aggregation rule; at the moment four such
rules are available:
'fMax': family maximum,
'algSum': family Algebraic Sum,
'eSum': family Einstein Sum,
'hSum': family Hamacher Sum
default &mdash;&gt; 'fMax'</dd>
<dt><strong><code>defuzz_method</code></strong> :&ensp;<code>str,
</code></dt>
<dd>defuzzification method; at the moment four such
rules are available:
'centroid': Centroid,
'bisector': Bisector,
'mom': MeanOfMax,
'som': MinOfMax,
'lom' : MaxOfMax
default &mdash;&gt; 'centroid'</dd>
</dl>
<h2 id="return">Return</h2>
<p>y: pd.DataFrame
the connection matrix with the defuzzified values</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@type_check
def build(self, data: collections.OrderedDict, implication_method:str=&#39;Mamdani&#39;, 
                aggregation_method:str=&#39;fMax&#39;, defuzz_method:str=&#39;centroid&#39;) -&gt; pd.DataFrame:
    &#34;&#34;&#34;
        Build an FCM based on qualitative input data.

        Parameters
        ----------
        data: collections.OrderedDict
                ordered dictionary with the qualitative input data.
        
        implication_method: str,
                            implication rule; at the moment two such
                            rules are available;
                                &#39;Mamdani&#39;: minimum implication rule
                                &#39;Larsen&#39;: product implication rule
                                default ---&gt; &#39;Mamdani&#39;

        aggregation_method: str,
                            aggregation rule; at the moment four such
                            rules are available:
                                &#39;fMax&#39;: family maximum,
                                &#39;algSum&#39;: family Algebraic Sum,
                                &#39;eSum&#39;: family Einstein Sum,
                                &#39;hSum&#39;: family Hamacher Sum
                                default ---&gt; &#39;fMax&#39;

        defuzz_method: str,            
                        defuzzification method; at the moment four such
                        rules are available:
                            &#39;centroid&#39;: Centroid,
                            &#39;bisector&#39;: Bisector,
                            &#39;mom&#39;: MeanOfMax,
                            &#39;som&#39;: MinOfMax,
                            &#39;lom&#39; : MaxOfMax
                            default ---&gt; &#39;centroid&#39;

        Return
        -------
        y: pd.DataFrame
            the connection matrix with the defuzzified values            
    &#34;&#34;&#34;
    if self.fuzzy_membership is None:
        raise ValueError(&#39;Membership function is not defined!&#39;)

    if self.linguistic_terms is None:
        raise ValueError(&#39;linguistic_terms are not defined!&#39;)

    if self.universe is None:
        raise ValueError(&#39;Universe of discourse is not defined!&#39;)

    nExperts = len(data)

    # Drop the columns that should be omitted from the calculations (e.g., unsure)
    keep = [i.lower() for i in list(self.linguistic_terms.keys())]
    flat_data = Transform.flatData(data)
    flat_data = flat_data[keep]
    
    # Create an empty weight matrix
    cols = set(flat_data.index.get_level_values(&#39;to&#39;))
    index = set(flat_data.index.get_level_values(&#39;from&#39;))
    index = sorted(index.union(cols))
    weight_matrix = pd.DataFrame(0,columns=index, index=index)
    
    # main part for calculating the weights
    for concepts in set(flat_data.index):
        # for a given pair of concepts calculate the propostions (weights) for the
        # implication rules.
        activation_parameter = Transform.calculateProportions(data=flat_data, 
                                        conceptPair=concepts, nExperts=nExperts)
        activated = {}
        # for each linguistic term apply the implication rule
        for term in self.fuzzy_membership.keys():
            act = self.fuzzy_implication(membership_function=self.fuzzy_membership[term], 
                                        weight=activation_parameter[term], method=implication_method)
            activated[term] = act
        
        # if the &#39;activated&#39; membership functions are not all zeros then aggregate 
        # them and defuzzify them.
        if not all(x==0 for x in activation_parameter.values()):
            # aggregate all the activated membership functions
            aggregated = functools.reduce(lambda x,y: self.aggregate(x=x, y=y, method=aggregation_method),
                                            [activated[i] for i in activated.keys()])

            # defuzzify the aggregated functions                                
            value = self.defuzz(x=self.universe, mfx=aggregated, method=defuzz_method)
            # populate the empty weigtht_matrix with the defuzzified value
            weight_matrix.loc[concepts] = value
    
    weight_matrix = weight_matrix.fillna(0)

    return weight_matrix</code></pre>
</details>
</dd>
<dt id="fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.defuzz"><code class="name flex">
<span>def <span class="ident">defuzz</span></span>(<span>self, x, mfx, method: str = 'centroid', **kwargs) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Defuzzification of the aggregated membership functions.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>universe of discourse</dd>
<dt><strong><code>mfx</code></strong> :&ensp;<code>numpy.ndarray,</code></dt>
<dd>"aggregated" membership functions</dd>
<dt><strong><code>method</code></strong> :&ensp;<code>str,</code></dt>
<dd>defuzzification method; at the moment four such
rules are available:
'centroid': Centroid,
'bisector': Bisector,
'mom': MeanOfMax,
'som': MinOfMax,
'lom' : MaxOfMax
default &mdash;&gt; 'centroid'</dd>
</dl>
<h2 id="return">Return</h2>
<p>y: float
defuzzified value</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@type_check
def defuzz(self, x, mfx, method:str=&#39;centroid&#39;, **kwargs) -&gt; float:
    &#34;&#34;&#34;
        Defuzzification of the aggregated membership functions.

        Parameters
        ----------
        x: numpy.ndarray
            universe of discourse 
        
        mfx: numpy.ndarray,
                    &#34;aggregated&#34; membership functions
        
        method: str,
                defuzzification method; at the moment four such 
                rules are available:
                    &#39;centroid&#39;: Centroid,
                    &#39;bisector&#39;: Bisector,
                    &#39;mom&#39;: MeanOfMax,
                    &#39;som&#39;: MinOfMax,
                    &#39;lom&#39; : MaxOfMax
                    default ---&gt; &#39;centroid&#39;

        Return
        -------
        y: float
            defuzzified value
    &#34;&#34;&#34;

    defuzz = DefuzzStore.get(method)()
    defuzz_value = defuzz.defuzz(x=x, mfx=mfx, method=method, params=kwargs)

    return defuzz_value</code></pre>
</details>
</dd>
<dt id="fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.entropy"><code class="name flex">
<span>def <span class="ident">entropy</span></span>(<span>self, data: collections.OrderedDict, method: str = 'entropy', **kwargs) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the entropy of the expert ratings.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>collections.OrderedDict</code></dt>
<dd>ordered dictionary with the expert inputs</dd>
<dt><strong><code>method</code></strong> :&ensp;<code>str</code></dt>
<dd>method for calculating entropy. At the moment only information entropy is available
default &ndash;&gt; 'entropy'</dd>
</dl>
<h2 id="return">Return</h2>
<p>y: pandas.DataFrame,
entropy of the concept pairs in expert ratings</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@type_check
def entropy(self, data: collections.OrderedDict, 
            method:str = &#39;entropy&#39;, **kwargs) -&gt; pd.DataFrame:
    &#34;&#34;&#34;
        Calculate the entropy of the expert ratings.

        Parameters
        ----------
        data: collections.OrderedDict
            ordered dictionary with the expert inputs
        
        method: str
                method for calculating entropy. At the moment only information entropy is available
                default --&gt; &#39;entropy&#39;

        Return
        -------
        y: pandas.DataFrame,
            entropy of the concept pairs in expert ratings
    &#34;&#34;&#34;

    entropy = EntropyStore.get(method)()
    return entropy.calculateEntropy(data=data, params=kwargs)</code></pre>
</details>
</dd>
<dt id="fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.fuzzy_implication"><code class="name flex">
<span>def <span class="ident">fuzzy_implication</span></span>(<span>self, membership_function, weight, method: str = 'Mamdani', **kwargs) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Fuzzy implication rule.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>membership_function</code></strong> :&ensp;<code>numpy.ndarray,</code></dt>
<dd>membership function of a linguistic term (x)</dd>
<dt><strong><code>weight</code></strong> :&ensp;<code>float,</code></dt>
<dd>the weight at which the given membership function x should be "activated"
(i.e., the cut point or the point at which the membership function should be rescaled)</dd>
<dt><strong><code>method</code></strong> :&ensp;<code>str,</code></dt>
<dd>implication rule; at the moment two such rules are available
'Mamdani': minimum implication rule
'Larsen': product implication rule
default &mdash;&gt; 'Mamdani'</dd>
</dl>
<h2 id="return">Return</h2>
<p>y: numpy.ndarray
the "activated" membership function</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@type_check
def fuzzy_implication(self, membership_function, weight, 
                        method:str=&#39;Mamdani&#39;, **kwargs) -&gt; np.ndarray:
    &#34;&#34;&#34;
        Fuzzy implication rule.

        Parameters
        ----------
        membership_function: numpy.ndarray,
                                membership function of a linguistic term (x)

        weight: float,
                    the weight at which the given membership function x should be &#34;activated&#34; 
                    (i.e., the cut point or the point at which the membership function should be rescaled)
        
        method: str,
                implication rule; at the moment two such rules are available 
                &#39;Mamdani&#39;: minimum implication rule
                &#39;Larsen&#39;: product implication rule
                default ---&gt; &#39;Mamdani&#39;

        Return
        -------
        y: numpy.ndarray
            the &#34;activated&#34; membership function
    &#34;&#34;&#34;
    implication = ImplicationStore.get(method)()

    return implication.implication(mf_x = membership_function, 
                                    weight = weight, params = kwargs)</code></pre>
</details>
</dd>
<dt id="fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.read_data"><code class="name flex">
<span>def <span class="ident">read_data</span></span>(<span>self, file_path: str, **kwargs) ‑> collections.OrderedDict</span>
</code></dt>
<dd>
<div class="desc"><p>Read data from a file. Currently, the method supports
.csv, .xlsx and .json file formats.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_path</code></strong> :&ensp;<code>str </code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="other-parameters">Other Parameters</h2>
<p>for .csv files:</p>
<pre><code>**sep_concept: str,
                separation symbol (e.g., '-&gt;') that separates the antecedent 
                from the consequent in the column heads of a csv file
                default ---&gt; '-&gt;'

**csv_sep: str,
            separator of the csv file
            default ---&gt; ','
</code></pre>
<p>for .xlsx files:</p>
<pre><code>**check_consistency: Bool
                    check the consistency of raitings across the experts
                    default --&gt; False

**engine: str,
            the engine for excel reader (read more in pd.read_excel)
            default --&gt; "openpyxl"
</code></pre>
<p>for .json files:</p>
<pre><code>**check_consistency: Bool
                    check the consistency of raitings across the experts.
                    default --&gt; False
</code></pre>
<h2 id="return">Return</h2>
<p>data: collections.OrderedDict
ordered dictionary with the formatted data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@type_check
def read_data(self, file_path:str, **kwargs) -&gt; collections.OrderedDict:
    
    &#34;&#34;&#34; 
        Read data from a file. Currently, the method supports 
        .csv, .xlsx and .json file formats.
        
        Parameters
        ----------
        file_path : str 
        
        Other Parameters
        ----------------
        for .csv files:

            **sep_concept: str,
                            separation symbol (e.g., &#39;-&gt;&#39;) that separates the antecedent 
                            from the consequent in the column heads of a csv file
                            default ---&gt; &#39;-&gt;&#39;

            **csv_sep: str,
                        separator of the csv file
                        default ---&gt; &#39;,&#39;
                        
        for .xlsx files:

            **check_consistency: Bool
                                check the consistency of raitings across the experts
                                default --&gt; False

            **engine: str,
                        the engine for excel reader (read more in pd.read_excel)
                        default --&gt; &#34;openpyxl&#34;

        for .json files:

            **check_consistency: Bool
                                check the consistency of raitings across the experts.
                                default --&gt; False
        
        Return
        -------
        data: collections.OrderedDict
                ordered dictionary with the formatted data.
    &#34;&#34;&#34;

    if self.linguistic_terms is None:
        raise ValueError(&#39;Linguistic terms are not defined!&#39;)

    fileType = file_path.split(&#39;.&#39;)[-1]
    reader = ReaderStore.get(fileType)()
    data = reader.read(filePath=file_path, linguisticTerms = self.linguistic_terms,
                        params = kwargs)
    
    return data</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="fcmpy.expert_fcm.expert_based_fcm.FcmConstructor"><code class="flex name class">
<span>class <span class="ident">FcmConstructor</span></span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FcmConstructor(ABC):
    @abstractmethod
    def read_data(file_path, **kwargs) -&gt; collections.OrderedDict:
        raise NotImplementedError(&#39;read_data method is not defined!&#39;)

    @abstractmethod
    def build(data: collections.OrderedDict, implication_method:str, 
                    aggregation_method:str, defuzz_method:str) -&gt; pd.DataFrame:
                    
        raise NotImplementedError(&#39;build method is not defined!&#39;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="fcmpy.expert_fcm.expert_based_fcm.ExpertFcm" href="#fcmpy.expert_fcm.expert_based_fcm.ExpertFcm">ExpertFcm</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="fcmpy.expert_fcm.expert_based_fcm.FcmConstructor.build"><code class="name flex">
<span>def <span class="ident">build</span></span>(<span>data: collections.OrderedDict, implication_method: str, aggregation_method: str, defuzz_method: str) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def build(data: collections.OrderedDict, implication_method:str, 
                aggregation_method:str, defuzz_method:str) -&gt; pd.DataFrame:
                
    raise NotImplementedError(&#39;build method is not defined!&#39;)</code></pre>
</details>
</dd>
<dt id="fcmpy.expert_fcm.expert_based_fcm.FcmConstructor.read_data"><code class="name flex">
<span>def <span class="ident">read_data</span></span>(<span>file_path, **kwargs) ‑> collections.OrderedDict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def read_data(file_path, **kwargs) -&gt; collections.OrderedDict:
    raise NotImplementedError(&#39;read_data method is not defined!&#39;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="fcmpy.expert_fcm" href="index.html">fcmpy.expert_fcm</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="fcmpy.expert_fcm.expert_based_fcm.ExpertFcm" href="#fcmpy.expert_fcm.expert_based_fcm.ExpertFcm">ExpertFcm</a></code></h4>
<ul class="two-column">
<li><code><a title="fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.aggregate" href="#fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.aggregate">aggregate</a></code></li>
<li><code><a title="fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.automf" href="#fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.automf">automf</a></code></li>
<li><code><a title="fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.build" href="#fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.build">build</a></code></li>
<li><code><a title="fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.defuzz" href="#fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.defuzz">defuzz</a></code></li>
<li><code><a title="fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.entropy" href="#fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.entropy">entropy</a></code></li>
<li><code><a title="fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.fuzzy_implication" href="#fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.fuzzy_implication">fuzzy_implication</a></code></li>
<li><code><a title="fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.fuzzy_membership" href="#fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.fuzzy_membership">fuzzy_membership</a></code></li>
<li><code><a title="fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.linguistic_terms" href="#fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.linguistic_terms">linguistic_terms</a></code></li>
<li><code><a title="fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.read_data" href="#fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.read_data">read_data</a></code></li>
<li><code><a title="fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.universe" href="#fcmpy.expert_fcm.expert_based_fcm.ExpertFcm.universe">universe</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="fcmpy.expert_fcm.expert_based_fcm.FcmConstructor" href="#fcmpy.expert_fcm.expert_based_fcm.FcmConstructor">FcmConstructor</a></code></h4>
<ul class="">
<li><code><a title="fcmpy.expert_fcm.expert_based_fcm.FcmConstructor.build" href="#fcmpy.expert_fcm.expert_based_fcm.FcmConstructor.build">build</a></code></li>
<li><code><a title="fcmpy.expert_fcm.expert_based_fcm.FcmConstructor.read_data" href="#fcmpy.expert_fcm.expert_based_fcm.FcmConstructor.read_data">read_data</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>